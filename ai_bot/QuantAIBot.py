# -*- coding: utf-8 -*-
from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline
import gradio as gr

class QuantAIBot:
    def __init__(self, model_name="bert-base-uncased"):
        """
        åˆå§‹åŒ– Quant AI æœºå™¨äººï¼ŒåŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ ğŸš€
        """
        print("ğŸš€ æ­£åœ¨åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨...")
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForQuestionAnswering.from_pretrained(model_name)
        self.qa_pipeline = pipeline("question-answering", model=self.model, tokenizer=self.tokenizer)
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼")

    def answer_question(self, question, context):
        """
        å›ç­”ç”¨æˆ·çš„é—®é¢˜ ğŸ¤–
        """
        print(f"ğŸ¤– ç”¨æˆ·é—®é¢˜: {question}")
        print(f"ğŸ“š ä¸Šä¸‹æ–‡: {context}")
        result = self.qa_pipeline(question=question, context=context)
        print(f"ğŸ‰ å›ç­”: {result['answer']}")
        return result['answer']

    def run_gradio_app(self):
        """
        å¯åŠ¨ Gradio åº”ç”¨ï¼Œæä¾›ç”¨æˆ·ç•Œé¢ ğŸ–¥ï¸
        """
        print("ğŸ–¥ï¸ å¯åŠ¨ Gradio åº”ç”¨...")
        iface = gr.Interface(
            fn=self.answer_question,
            inputs=["text", "text"],
            outputs="text",
            title="ğŸ“Š Quant AI æœºå™¨äºº",
            description="è¯·è¾“å…¥ä½ çš„é‡åŒ–ç›¸å…³é—®é¢˜ï¼Œæˆ‘ä¼šå°½åŠ›å›ç­”ï¼ğŸ¤–"
        )
        iface.launch()

# ä¸»ç¨‹åºå…¥å£
if __name__ == "__main__":
    # åˆå§‹åŒ– Quant AI æœºå™¨äºº
    bot = QuantAIBot(model_name="bert-base-uncased")  # å¯ä»¥ä½¿ç”¨å…¶ä»–æ¨¡å‹ï¼Œå¦‚ "bert-base-chinese"

    # å¯åŠ¨ Gradio åº”ç”¨
    bot.run_gradio_app()
